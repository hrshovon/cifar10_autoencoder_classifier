{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D, BatchNormalization,Dropout,Flatten,Dense, Input, Conv2DTranspose, Activation, UpSampling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.initializers import glorot_normal\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.metrics as mets\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#explore the dataset\n",
    "data_label_dict={\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"drog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}\n",
    "random.seed(15)\n",
    "(trX,trY),(tsX,tsY)=cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    inp=Input(shape=trX[0,:,:,:].shape)\n",
    "    x=Conv2D(filters=96, kernel_size=(3,3))(inp)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Conv2D(filters=96, kernel_size=(3,3), strides=2)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    x=Conv2D(filters=192, kernel_size=(3,3))(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Conv2D(filters=192, kernel_size=(3,3), strides=2)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dropout(0.5)(x)\n",
    "    x=Flatten()(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Dense(256)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dense(10, activation=\"softmax\")(x)\n",
    "    model=Model(inp,x)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  0\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 7s 148us/sample - loss: 1.9446 - acc: 0.3121 - val_loss: 1.6937 - val_acc: 0.3820\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 6s 144us/sample - loss: 1.5168 - acc: 0.4468 - val_loss: 1.7052 - val_acc: 0.4012\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 6s 136us/sample - loss: 1.3211 - acc: 0.5249 - val_loss: 1.4189 - val_acc: 0.5000\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 6s 135us/sample - loss: 1.1853 - acc: 0.5770 - val_loss: 1.3246 - val_acc: 0.5512\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 6s 136us/sample - loss: 1.0569 - acc: 0.6240 - val_loss: 1.0712 - val_acc: 0.6210\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 6s 135us/sample - loss: 0.9540 - acc: 0.6601 - val_loss: 0.9918 - val_acc: 0.6586\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 6s 134us/sample - loss: 0.8700 - acc: 0.6913 - val_loss: 0.9659 - val_acc: 0.6662\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 0.7933 - acc: 0.7171 - val_loss: 0.8367 - val_acc: 0.7126\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.7296 - acc: 0.7415 - val_loss: 0.7916 - val_acc: 0.7230\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 0.6722 - acc: 0.7617 - val_loss: 0.8475 - val_acc: 0.7126\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 0.6236 - acc: 0.7793 - val_loss: 0.7618 - val_acc: 0.7394\n",
      "Epoch 12/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.5735 - acc: 0.7969 - val_loss: 0.7530 - val_acc: 0.7440\n",
      "Epoch 13/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 0.5311 - acc: 0.8103 - val_loss: 0.8064 - val_acc: 0.7304\n",
      "Epoch 14/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.5088 - acc: 0.8194 - val_loss: 0.7493 - val_acc: 0.7442\n",
      "Epoch 15/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.4631 - acc: 0.8358 - val_loss: 0.7387 - val_acc: 0.7552\n",
      "Epoch 16/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.4419 - acc: 0.8441 - val_loss: 0.7915 - val_acc: 0.7562\n",
      "Epoch 17/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.4030 - acc: 0.8557 - val_loss: 0.7616 - val_acc: 0.7576\n",
      "Epoch 18/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.3864 - acc: 0.8620 - val_loss: 0.7716 - val_acc: 0.7528\n",
      "Epoch 19/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.3597 - acc: 0.8706 - val_loss: 0.6979 - val_acc: 0.7702\n",
      "Epoch 20/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.3360 - acc: 0.8797 - val_loss: 0.8666 - val_acc: 0.7540\n",
      "Epoch 21/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.3190 - acc: 0.8860 - val_loss: 0.7725 - val_acc: 0.7698\n",
      "Epoch 22/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.3040 - acc: 0.8901 - val_loss: 0.7414 - val_acc: 0.7720\n",
      "Epoch 23/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.2972 - acc: 0.8936 - val_loss: 0.7578 - val_acc: 0.7690\n",
      "Epoch 24/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.2771 - acc: 0.9008 - val_loss: 0.7758 - val_acc: 0.7706\n",
      "Epoch 25/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.2593 - acc: 0.9071 - val_loss: 0.7449 - val_acc: 0.7796\n",
      "fold  1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 7s 145us/sample - loss: 1.9233 - acc: 0.3162 - val_loss: 1.7126 - val_acc: 0.3628\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 1.5051 - acc: 0.4486 - val_loss: 1.7752 - val_acc: 0.3724\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 1.3225 - acc: 0.5249 - val_loss: 1.2883 - val_acc: 0.5310\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 1.1815 - acc: 0.5784 - val_loss: 1.2166 - val_acc: 0.5634\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 1.0690 - acc: 0.6208 - val_loss: 1.1811 - val_acc: 0.5894\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.9701 - acc: 0.6565 - val_loss: 1.0649 - val_acc: 0.6324\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.8865 - acc: 0.6845 - val_loss: 0.8916 - val_acc: 0.6834\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.7959 - acc: 0.7164 - val_loss: 0.8501 - val_acc: 0.7060\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 0.7220 - acc: 0.7471 - val_loss: 0.8105 - val_acc: 0.7128\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.6683 - acc: 0.7631 - val_loss: 0.8079 - val_acc: 0.7202\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.6109 - acc: 0.7829 - val_loss: 0.7986 - val_acc: 0.7228\n",
      "Epoch 12/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.5672 - acc: 0.7985 - val_loss: 0.8074 - val_acc: 0.7270\n",
      "Epoch 13/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.5257 - acc: 0.8146 - val_loss: 0.7502 - val_acc: 0.7442\n",
      "Epoch 14/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.5000 - acc: 0.8228 - val_loss: 0.7983 - val_acc: 0.7334\n",
      "Epoch 15/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.4614 - acc: 0.8353 - val_loss: 0.9505 - val_acc: 0.6998\n",
      "Epoch 16/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.4453 - acc: 0.8404 - val_loss: 0.7529 - val_acc: 0.7468\n",
      "Epoch 17/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.4064 - acc: 0.8544 - val_loss: 0.7536 - val_acc: 0.7578\n",
      "Epoch 18/25\n",
      "45000/45000 [==============================] - 6s 142us/sample - loss: 0.3723 - acc: 0.8654 - val_loss: 0.8130 - val_acc: 0.7536\n",
      "Epoch 19/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.3507 - acc: 0.8741 - val_loss: 1.3957 - val_acc: 0.7458\n",
      "Epoch 20/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.3307 - acc: 0.8823 - val_loss: 0.8053 - val_acc: 0.7542\n",
      "Epoch 21/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.3050 - acc: 0.8888 - val_loss: 0.7905 - val_acc: 0.7626\n",
      "Epoch 22/25\n",
      "45000/45000 [==============================] - 6s 135us/sample - loss: 0.2869 - acc: 0.8954 - val_loss: 0.8339 - val_acc: 0.7558\n",
      "Epoch 23/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.2698 - acc: 0.9036 - val_loss: 0.8232 - val_acc: 0.7610\n",
      "Epoch 24/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.2574 - acc: 0.9075 - val_loss: 0.8580 - val_acc: 0.7520\n",
      "Epoch 25/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 0.2580 - acc: 0.9086 - val_loss: 0.8364 - val_acc: 0.7586\n",
      "fold  2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 7s 148us/sample - loss: 1.8899 - acc: 0.3308 - val_loss: 2.0177 - val_acc: 0.2742\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 1.4417 - acc: 0.4767 - val_loss: 1.5226 - val_acc: 0.4490\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 1.2598 - acc: 0.5488 - val_loss: 1.5015 - val_acc: 0.4902\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 1.1320 - acc: 0.5944 - val_loss: 1.2172 - val_acc: 0.5868\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 1.0210 - acc: 0.6393 - val_loss: 0.9778 - val_acc: 0.6496\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.9047 - acc: 0.6794 - val_loss: 1.1404 - val_acc: 0.6406\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.8308 - acc: 0.7056 - val_loss: 0.8554 - val_acc: 0.6958\n",
      "Epoch 8/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.7565 - acc: 0.7328 - val_loss: 0.8485 - val_acc: 0.7036\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.7060 - acc: 0.7502 - val_loss: 0.8410 - val_acc: 0.7110\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.6622 - acc: 0.7664 - val_loss: 0.7588 - val_acc: 0.7322\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.6101 - acc: 0.7851 - val_loss: 0.8060 - val_acc: 0.7196\n",
      "Epoch 12/25\n",
      "45000/45000 [==============================] - 6s 136us/sample - loss: 0.5608 - acc: 0.8005 - val_loss: 0.8596 - val_acc: 0.7238\n",
      "Epoch 13/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.5366 - acc: 0.8095 - val_loss: 0.8999 - val_acc: 0.7298\n",
      "Epoch 14/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 0.4967 - acc: 0.8244 - val_loss: 0.7173 - val_acc: 0.7580\n",
      "Epoch 15/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.4477 - acc: 0.8405 - val_loss: 0.8069 - val_acc: 0.7410\n",
      "Epoch 16/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.4098 - acc: 0.8548 - val_loss: 0.7704 - val_acc: 0.7580\n",
      "Epoch 17/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.3884 - acc: 0.8616 - val_loss: 0.7518 - val_acc: 0.7532\n",
      "Epoch 18/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.3684 - acc: 0.8683 - val_loss: 0.7922 - val_acc: 0.7464\n",
      "Epoch 19/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.3447 - acc: 0.8768 - val_loss: 0.9077 - val_acc: 0.7268\n",
      "Epoch 20/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.3333 - acc: 0.8808 - val_loss: 0.7545 - val_acc: 0.7628\n",
      "Epoch 21/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.3036 - acc: 0.8915 - val_loss: 0.7861 - val_acc: 0.7614\n",
      "Epoch 22/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.2922 - acc: 0.8958 - val_loss: 0.7653 - val_acc: 0.7710\n",
      "Epoch 23/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.2729 - acc: 0.9024 - val_loss: 0.8531 - val_acc: 0.7550\n",
      "Epoch 24/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.2539 - acc: 0.9088 - val_loss: 0.8076 - val_acc: 0.7758\n",
      "Epoch 25/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.2506 - acc: 0.9105 - val_loss: 1.0385 - val_acc: 0.7534\n",
      "fold  3\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 7s 148us/sample - loss: 1.8646 - acc: 0.3428 - val_loss: 2.0222 - val_acc: 0.3316\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 1.4152 - acc: 0.4868 - val_loss: 1.5350 - val_acc: 0.4928\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 1.2258 - acc: 0.5602 - val_loss: 1.3293 - val_acc: 0.5364\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 1.0827 - acc: 0.6139 - val_loss: 1.1637 - val_acc: 0.5986\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.9536 - acc: 0.6629 - val_loss: 0.9869 - val_acc: 0.6476\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.8584 - acc: 0.6953 - val_loss: 0.9190 - val_acc: 0.6818\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.7720 - acc: 0.7288 - val_loss: 0.8039 - val_acc: 0.7198\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.7036 - acc: 0.7525 - val_loss: 0.7789 - val_acc: 0.7338\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.6570 - acc: 0.7685 - val_loss: 0.7931 - val_acc: 0.7252\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.5960 - acc: 0.7880 - val_loss: 0.8932 - val_acc: 0.7298\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.5496 - acc: 0.8064 - val_loss: 0.7507 - val_acc: 0.7480\n",
      "Epoch 12/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.5102 - acc: 0.8179 - val_loss: 0.7612 - val_acc: 0.7468\n",
      "Epoch 13/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.4825 - acc: 0.8286 - val_loss: 0.7803 - val_acc: 0.7456\n",
      "Epoch 14/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.4320 - acc: 0.8460 - val_loss: 0.7412 - val_acc: 0.7598\n",
      "Epoch 15/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.4018 - acc: 0.8584 - val_loss: 0.7559 - val_acc: 0.7656\n",
      "Epoch 16/25\n",
      "45000/45000 [==============================] - 6s 136us/sample - loss: 0.3762 - acc: 0.8675 - val_loss: 0.9973 - val_acc: 0.7488\n",
      "Epoch 17/25\n",
      "45000/45000 [==============================] - 6s 132us/sample - loss: 0.3534 - acc: 0.8734 - val_loss: 0.8194 - val_acc: 0.7546\n",
      "Epoch 18/25\n",
      "45000/45000 [==============================] - 6s 133us/sample - loss: 0.3441 - acc: 0.8773 - val_loss: 0.8001 - val_acc: 0.7610\n",
      "Epoch 19/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.3076 - acc: 0.8906 - val_loss: 0.7959 - val_acc: 0.7704\n",
      "Epoch 20/25\n",
      "45000/45000 [==============================] - 6s 136us/sample - loss: 0.2927 - acc: 0.8955 - val_loss: 0.9937 - val_acc: 0.7526\n",
      "Epoch 21/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.2815 - acc: 0.8994 - val_loss: 0.7975 - val_acc: 0.7786\n",
      "Epoch 22/25\n",
      "45000/45000 [==============================] - 6s 136us/sample - loss: 0.2598 - acc: 0.9059 - val_loss: 0.7960 - val_acc: 0.7672\n",
      "Epoch 23/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.2471 - acc: 0.9123 - val_loss: 0.8607 - val_acc: 0.7664\n",
      "Epoch 24/25\n",
      "45000/45000 [==============================] - 6s 135us/sample - loss: 0.2459 - acc: 0.9124 - val_loss: 0.8553 - val_acc: 0.7712\n",
      "Epoch 25/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.2331 - acc: 0.9185 - val_loss: 0.8644 - val_acc: 0.7774\n",
      "fold  4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 7s 148us/sample - loss: 1.9187 - acc: 0.3243 - val_loss: 1.8270 - val_acc: 0.3396\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 1.4483 - acc: 0.4744 - val_loss: 1.5349 - val_acc: 0.4680\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 1.2688 - acc: 0.5452 - val_loss: 1.2577 - val_acc: 0.5582\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 1.1181 - acc: 0.6018 - val_loss: 1.0688 - val_acc: 0.6160\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 1.0012 - acc: 0.6440 - val_loss: 1.0067 - val_acc: 0.6400\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.8888 - acc: 0.6859 - val_loss: 0.9671 - val_acc: 0.6650\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.8066 - acc: 0.7142 - val_loss: 0.8589 - val_acc: 0.7024\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.7388 - acc: 0.7394 - val_loss: 0.7832 - val_acc: 0.7212\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.6790 - acc: 0.7594 - val_loss: 0.7790 - val_acc: 0.7198\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 6s 135us/sample - loss: 0.6243 - acc: 0.7786 - val_loss: 0.7952 - val_acc: 0.7182\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 7s 145us/sample - loss: 0.5756 - acc: 0.7961 - val_loss: 0.8926 - val_acc: 0.7008\n",
      "Epoch 12/25\n",
      "45000/45000 [==============================] - 6s 143us/sample - loss: 0.5272 - acc: 0.8113 - val_loss: 0.7575 - val_acc: 0.7464\n",
      "Epoch 13/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.5029 - acc: 0.8221 - val_loss: 0.7648 - val_acc: 0.7464\n",
      "Epoch 14/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.4617 - acc: 0.8353 - val_loss: 0.8033 - val_acc: 0.7384\n",
      "Epoch 15/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 6s 143us/sample - loss: 0.4230 - acc: 0.8488 - val_loss: 0.7409 - val_acc: 0.7554\n",
      "Epoch 16/25\n",
      "45000/45000 [==============================] - 6s 144us/sample - loss: 0.4074 - acc: 0.8545 - val_loss: 0.8225 - val_acc: 0.7384\n",
      "Epoch 17/25\n",
      "45000/45000 [==============================] - 6s 143us/sample - loss: 0.3770 - acc: 0.8648 - val_loss: 0.7815 - val_acc: 0.7482\n",
      "Epoch 18/25\n",
      "45000/45000 [==============================] - 6s 143us/sample - loss: 0.3531 - acc: 0.8734 - val_loss: 0.7890 - val_acc: 0.7516\n",
      "Epoch 19/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.3236 - acc: 0.8842 - val_loss: 0.7653 - val_acc: 0.7576\n",
      "Epoch 20/25\n",
      "45000/45000 [==============================] - 6s 142us/sample - loss: 0.3056 - acc: 0.8897 - val_loss: 0.8118 - val_acc: 0.7582\n",
      "Epoch 21/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.3171 - acc: 0.8858 - val_loss: 0.9005 - val_acc: 0.7370\n",
      "Epoch 22/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.2893 - acc: 0.8962 - val_loss: 0.8189 - val_acc: 0.7580\n",
      "Epoch 23/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 0.2708 - acc: 0.9032 - val_loss: 0.8661 - val_acc: 0.7464\n",
      "Epoch 24/25\n",
      "45000/45000 [==============================] - 6s 135us/sample - loss: 0.2560 - acc: 0.9091 - val_loss: 0.7930 - val_acc: 0.7614\n",
      "Epoch 25/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 0.2447 - acc: 0.9131 - val_loss: 0.7978 - val_acc: 0.7648\n",
      "fold  5\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 7s 150us/sample - loss: 1.8865 - acc: 0.3316 - val_loss: 1.6507 - val_acc: 0.4112\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 7s 147us/sample - loss: 1.4317 - acc: 0.4833 - val_loss: 1.5068 - val_acc: 0.4628\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 7s 146us/sample - loss: 1.2366 - acc: 0.5588 - val_loss: 1.2788 - val_acc: 0.5582\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 1.0964 - acc: 0.6106 - val_loss: 1.0756 - val_acc: 0.6266\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 6s 144us/sample - loss: 0.9746 - acc: 0.6551 - val_loss: 1.0162 - val_acc: 0.6576\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 6s 142us/sample - loss: 0.8849 - acc: 0.6874 - val_loss: 0.8537 - val_acc: 0.7096\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 7s 145us/sample - loss: 0.8143 - acc: 0.7100 - val_loss: 0.8080 - val_acc: 0.7184\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.7398 - acc: 0.7379 - val_loss: 0.7944 - val_acc: 0.7254\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 6s 142us/sample - loss: 0.6713 - acc: 0.7633 - val_loss: 0.7810 - val_acc: 0.7340\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 6s 142us/sample - loss: 0.6171 - acc: 0.7802 - val_loss: 0.8245 - val_acc: 0.7436\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 7s 148us/sample - loss: 0.5766 - acc: 0.7929 - val_loss: 0.7168 - val_acc: 0.7576\n",
      "Epoch 12/25\n",
      "45000/45000 [==============================] - 7s 145us/sample - loss: 0.5274 - acc: 0.8116 - val_loss: 0.7110 - val_acc: 0.7652\n",
      "Epoch 13/25\n",
      "45000/45000 [==============================] - 6s 142us/sample - loss: 0.4814 - acc: 0.8283 - val_loss: 0.7050 - val_acc: 0.7650\n",
      "Epoch 14/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.4570 - acc: 0.8362 - val_loss: 0.7399 - val_acc: 0.7462\n",
      "Epoch 15/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.4167 - acc: 0.8510 - val_loss: 0.7376 - val_acc: 0.7512\n",
      "Epoch 16/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.3915 - acc: 0.8614 - val_loss: 0.7383 - val_acc: 0.7638\n",
      "Epoch 17/25\n",
      "45000/45000 [==============================] - 6s 143us/sample - loss: 0.3712 - acc: 0.8669 - val_loss: 0.7408 - val_acc: 0.7678\n",
      "Epoch 18/25\n",
      "45000/45000 [==============================] - 6s 142us/sample - loss: 0.3491 - acc: 0.8752 - val_loss: 0.6977 - val_acc: 0.7786\n",
      "Epoch 19/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.3241 - acc: 0.8842 - val_loss: 0.7481 - val_acc: 0.7710\n",
      "Epoch 20/25\n",
      "45000/45000 [==============================] - 6s 134us/sample - loss: 0.2979 - acc: 0.8923 - val_loss: 0.7339 - val_acc: 0.7726\n",
      "Epoch 21/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 0.2854 - acc: 0.8965 - val_loss: 0.7786 - val_acc: 0.7686\n",
      "Epoch 22/25\n",
      "45000/45000 [==============================] - 6s 142us/sample - loss: 0.2763 - acc: 0.9012 - val_loss: 0.7388 - val_acc: 0.7742\n",
      "Epoch 23/25\n",
      "45000/45000 [==============================] - 6s 144us/sample - loss: 0.2608 - acc: 0.9051 - val_loss: 0.8860 - val_acc: 0.7716\n",
      "Epoch 24/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.2510 - acc: 0.9099 - val_loss: 0.7609 - val_acc: 0.7764\n",
      "Epoch 25/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 0.2282 - acc: 0.9184 - val_loss: 0.7984 - val_acc: 0.7752\n",
      "fold  6\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 7s 147us/sample - loss: 1.8748 - acc: 0.3363 - val_loss: 1.7461 - val_acc: 0.3762\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 1.4391 - acc: 0.4776 - val_loss: 1.4001 - val_acc: 0.4866\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 1.2600 - acc: 0.5478 - val_loss: 1.3769 - val_acc: 0.5266\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 1.1155 - acc: 0.6022 - val_loss: 1.1169 - val_acc: 0.6030\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.9977 - acc: 0.6465 - val_loss: 1.0650 - val_acc: 0.6298\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.9029 - acc: 0.6798 - val_loss: 0.9037 - val_acc: 0.6774\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.8099 - acc: 0.7146 - val_loss: 0.8788 - val_acc: 0.6952\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.7286 - acc: 0.7427 - val_loss: 0.8445 - val_acc: 0.7042\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 6s 142us/sample - loss: 0.6758 - acc: 0.7588 - val_loss: 0.7932 - val_acc: 0.7260\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.6204 - acc: 0.7790 - val_loss: 1.2553 - val_acc: 0.6760\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.5705 - acc: 0.7973 - val_loss: 0.7970 - val_acc: 0.7262\n",
      "Epoch 12/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.5315 - acc: 0.8097 - val_loss: 0.7580 - val_acc: 0.7396\n",
      "Epoch 13/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.4958 - acc: 0.8228 - val_loss: 0.7705 - val_acc: 0.7422\n",
      "Epoch 14/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.4489 - acc: 0.8401 - val_loss: 0.8093 - val_acc: 0.7330\n",
      "Epoch 15/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.4188 - acc: 0.8500 - val_loss: 0.7470 - val_acc: 0.7554\n",
      "Epoch 16/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.3940 - acc: 0.8605 - val_loss: 0.9130 - val_acc: 0.7320\n",
      "Epoch 17/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.3755 - acc: 0.8646 - val_loss: 0.8133 - val_acc: 0.7480\n",
      "Epoch 18/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.3493 - acc: 0.8747 - val_loss: 0.7975 - val_acc: 0.7522\n",
      "Epoch 19/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.3212 - acc: 0.8846 - val_loss: 0.7885 - val_acc: 0.7624\n",
      "Epoch 20/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 0.3101 - acc: 0.8897 - val_loss: 0.8247 - val_acc: 0.7468\n",
      "Epoch 21/25\n",
      "45000/45000 [==============================] - 6s 139us/sample - loss: 0.2926 - acc: 0.8949 - val_loss: 0.8584 - val_acc: 0.7442\n",
      "Epoch 22/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 6s 142us/sample - loss: 0.2839 - acc: 0.8979 - val_loss: 0.8215 - val_acc: 0.7486\n",
      "Epoch 23/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 0.2674 - acc: 0.9037 - val_loss: 0.8143 - val_acc: 0.7590\n",
      "Epoch 24/25\n",
      "45000/45000 [==============================] - 6s 142us/sample - loss: 0.2443 - acc: 0.9119 - val_loss: 0.8091 - val_acc: 0.7664\n",
      "Epoch 25/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.2448 - acc: 0.9119 - val_loss: 0.8792 - val_acc: 0.7518\n",
      "fold  7\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 7s 148us/sample - loss: 1.9230 - acc: 0.3173 - val_loss: 1.8957 - val_acc: 0.3058\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 6s 144us/sample - loss: 1.4798 - acc: 0.4609 - val_loss: 1.7421 - val_acc: 0.3742\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 1.2701 - acc: 0.5424 - val_loss: 1.2544 - val_acc: 0.5492\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 6s 141us/sample - loss: 1.1231 - acc: 0.5984 - val_loss: 1.1236 - val_acc: 0.5920\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 6s 137us/sample - loss: 1.0094 - acc: 0.6412 - val_loss: 0.9946 - val_acc: 0.6486\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 6s 138us/sample - loss: 0.9090 - acc: 0.6770 - val_loss: 0.9957 - val_acc: 0.6470\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 6s 140us/sample - loss: 0.8329 - acc: 0.7066 - val_loss: 0.8603 - val_acc: 0.6998\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 6s 144us/sample - loss: 0.7533 - acc: 0.7326 - val_loss: 0.8589 - val_acc: 0.7136\n",
      "Epoch 9/25\n",
      "12032/45000 [=======>......................] - ETA: 4s - loss: 0.6952 - acc: 0.7570"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1979bc55b6de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel_tst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     H = model_tst.fit(trainX, trainY, validation_data=(valX,valY), \n\u001b[0;32m---> 15\u001b[0;31m                   epochs=n_epochs, batch_size=batch_size, callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_tst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mfold\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "batch_size = 256\n",
    "callbacks_list = None\n",
    "kf=StratifiedKFold(10,random_state=42)\n",
    "models=[]\n",
    "fold=0\n",
    "for train_idx,val_idx in kf.split(trX,trY):\n",
    "    print(\"fold \",fold)\n",
    "    trainX=trX[train_idx]\n",
    "    trainY=to_categorical(trY[train_idx],num_classes=10)\n",
    "    valX=trX[val_idx]\n",
    "    valY=to_categorical(trY[val_idx],num_classes=10)\n",
    "    model_tst=model()\n",
    "    H = model_tst.fit(trainX, trainY, validation_data=(valX,valY), \n",
    "                  epochs=n_epochs, batch_size=batch_size, callbacks=callbacks_list)\n",
    "    models.append(model_tst)\n",
    "    fold+=1\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 30, 30, 96)        2688      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 30, 30, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 96)        83040     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 14, 14, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 14, 14, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 12, 12, 192)       166080    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 12, 12, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 5, 5, 192)         331968    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 5, 5, 192)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5, 5, 192)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4800)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4800)              19200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               1229056   \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 1,834,602\n",
      "Trainable params: 1,825,002\n",
      "Non-trainable params: 9,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "W1222 01:40:48.234534 140547367667456 deprecation_wrapper.py:119] From /home/shovon/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1222 01:40:48.235515 140547367667456 deprecation_wrapper.py:119] From /home/shovon/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1222 01:40:48.236784 140547367667456 deprecation_wrapper.py:119] From /home/shovon/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1222 01:40:48.373138 140547367667456 deprecation_wrapper.py:119] From /home/shovon/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1222 01:40:48.373894 140547367667456 deprecation_wrapper.py:119] From /home/shovon/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1222 01:40:48.837731 140547367667456 deprecation_wrapper.py:119] From /home/shovon/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W1222 01:40:48.947008 140547367667456 deprecation_wrapper.py:119] From /home/shovon/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1222 01:40:48.952084 140547367667456 deprecation.py:506] From /home/shovon/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1222 01:40:49.343005 140547367667456 deprecation_wrapper.py:119] From /home/shovon/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1222 01:40:49.468322 140547367667456 deprecation.py:323] From /home/shovon/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "781/781 [==============================] - 18s 23ms/step - loss: 1.8796 - acc: 0.4352 - val_loss: 1.4411 - val_acc: 0.5605\n",
      "Epoch 2/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 1.2520 - acc: 0.6008 - val_loss: 1.0475 - val_acc: 0.6711\n",
      "Epoch 3/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 1.0711 - acc: 0.6567 - val_loss: 0.9201 - val_acc: 0.7184\n",
      "Epoch 4/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.9715 - acc: 0.6941 - val_loss: 0.9044 - val_acc: 0.7337\n",
      "Epoch 5/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.9033 - acc: 0.7184 - val_loss: 0.8206 - val_acc: 0.7574\n",
      "Epoch 6/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.8613 - acc: 0.7341 - val_loss: 0.7940 - val_acc: 0.7746\n",
      "Epoch 7/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.8268 - acc: 0.7503 - val_loss: 0.7859 - val_acc: 0.7746\n",
      "Epoch 8/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.8056 - acc: 0.7588 - val_loss: 0.7621 - val_acc: 0.7804\n",
      "Epoch 9/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.7819 - acc: 0.7688 - val_loss: 0.7812 - val_acc: 0.7832\n",
      "Epoch 10/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.7596 - acc: 0.7752 - val_loss: 0.7354 - val_acc: 0.7931\n",
      "Epoch 11/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.7448 - acc: 0.7838 - val_loss: 0.7161 - val_acc: 0.8013\n",
      "Epoch 12/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.7322 - acc: 0.7884 - val_loss: 0.7404 - val_acc: 0.7945\n",
      "Epoch 13/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.7211 - acc: 0.7934 - val_loss: 0.7151 - val_acc: 0.7968\n",
      "Epoch 14/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.7112 - acc: 0.7991 - val_loss: 0.6787 - val_acc: 0.8211\n",
      "Epoch 15/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6981 - acc: 0.8039 - val_loss: 0.6591 - val_acc: 0.8293\n",
      "Epoch 16/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6933 - acc: 0.8061 - val_loss: 0.6967 - val_acc: 0.8133\n",
      "Epoch 17/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6861 - acc: 0.8088 - val_loss: 0.6399 - val_acc: 0.8315\n",
      "Epoch 18/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6805 - acc: 0.8136 - val_loss: 0.6830 - val_acc: 0.8157\n",
      "Epoch 19/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6712 - acc: 0.8152 - val_loss: 0.6598 - val_acc: 0.8243\n",
      "Epoch 20/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6709 - acc: 0.8145 - val_loss: 0.6708 - val_acc: 0.8298\n",
      "Epoch 21/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6622 - acc: 0.8175 - val_loss: 0.6899 - val_acc: 0.8198\n",
      "Epoch 22/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6583 - acc: 0.8197 - val_loss: 0.7122 - val_acc: 0.8055\n",
      "Epoch 23/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6551 - acc: 0.8205 - val_loss: 0.6845 - val_acc: 0.8269\n",
      "Epoch 24/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6490 - acc: 0.8243 - val_loss: 0.6257 - val_acc: 0.8400\n",
      "Epoch 25/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6430 - acc: 0.8252 - val_loss: 0.6071 - val_acc: 0.8417\n",
      "Epoch 26/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6437 - acc: 0.8271 - val_loss: 0.7235 - val_acc: 0.8110\n",
      "Epoch 27/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6423 - acc: 0.8267 - val_loss: 0.6333 - val_acc: 0.8366\n",
      "Epoch 28/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6430 - acc: 0.8278 - val_loss: 0.6933 - val_acc: 0.8179\n",
      "Epoch 29/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6363 - acc: 0.8299 - val_loss: 0.6084 - val_acc: 0.8433\n",
      "Epoch 30/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6364 - acc: 0.8296 - val_loss: 0.6003 - val_acc: 0.8460\n",
      "Epoch 31/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6272 - acc: 0.8330 - val_loss: 0.6309 - val_acc: 0.8415\n",
      "Epoch 32/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6276 - acc: 0.8352 - val_loss: 0.5932 - val_acc: 0.8533\n",
      "Epoch 33/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6257 - acc: 0.8341 - val_loss: 0.5863 - val_acc: 0.8487\n",
      "Epoch 34/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6240 - acc: 0.8359 - val_loss: 0.6666 - val_acc: 0.8294\n",
      "Epoch 35/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6223 - acc: 0.8351 - val_loss: 0.6587 - val_acc: 0.8278\n",
      "Epoch 36/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6185 - acc: 0.8397 - val_loss: 0.5895 - val_acc: 0.8521\n",
      "Epoch 37/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6158 - acc: 0.8391 - val_loss: 0.6203 - val_acc: 0.8436\n",
      "Epoch 38/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6177 - acc: 0.8388 - val_loss: 0.5964 - val_acc: 0.8456\n",
      "Epoch 39/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6214 - acc: 0.8377 - val_loss: 0.6416 - val_acc: 0.8406\n",
      "Epoch 40/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6095 - acc: 0.8410 - val_loss: 0.5768 - val_acc: 0.8547\n",
      "Epoch 41/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6047 - acc: 0.8423 - val_loss: 0.5920 - val_acc: 0.8562\n",
      "Epoch 42/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6049 - acc: 0.8424 - val_loss: 0.6489 - val_acc: 0.8411\n",
      "Epoch 43/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6120 - acc: 0.8420 - val_loss: 0.6097 - val_acc: 0.8497\n",
      "Epoch 44/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6030 - acc: 0.8439 - val_loss: 0.6128 - val_acc: 0.8474\n",
      "Epoch 45/125\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6012 - acc: 0.8449 - val_loss: 0.6042 - val_acc: 0.8507\n",
      "Epoch 46/125\n",
      "388/781 [=============>................] - ETA: 7s - loss: 0.6008 - acc: 0.8452"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    " \n",
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    if epoch > 100:\n",
    "        lrate = 0.0003\n",
    "    return lrate\n",
    " \n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    " \n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    " \n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)\n",
    " \n",
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    " \n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "print(model.summary())\n",
    " \n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)\n",
    " \n",
    "#training\n",
    "batch_size = 64\n",
    " \n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
    "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
    "#save to disk\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('model.h5') \n",
    " \n",
    "#testing\n",
    "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
